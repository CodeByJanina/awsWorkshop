# AWS Data Lake Pipeline - Mimmit Koodaa Jumpstart Workshop

In this workshop, I built an end-to-end serverless data pipeline using AWS services as part of the *Mimmit Koodaa - Data Lake Jumpstart* workshop.

## Project Overview

This project demonstrates how to use various AWS services to create a scalable, serverless data lake architecture.

### Key Steps:

1. **Data Ingestion:** Captured raw data from various sources and stored it in Amazon S3.
2. **Data Transformation:** Defined and ran ETL jobs using AWS Glue to process the raw data into a structured format.
3. **Data Catalog:** Leveraged AWS Glue Data Catalog to manage metadata, including schema definitions and data lineage.
4. **Data Analysis:** Queried the transformed data with Amazon Athena using SQL syntax.
5. **Data Visualization:** Used Amazon QuickSight to create interactive visualizations and dashboards for data insights.

## AWS Services Used:

- **Amazon S3:** Central storage for raw and processed data.
- **AWS Glue:** Managed ETL service for data transformation and metadata management.
- **Amazon Athena:** Serverless query service for analyzing data with SQL.
- **Amazon QuickSight:** Business intelligence service for data visualization.

## Workshop Summary

This workshop highlighted the benefits of using serverless technologies in data lake architecture, eliminating the need for provisioning servers. Moving forward, I plan to explore additional services like AWS Lambda, Amazon Kinesis, and Amazon SageMaker to expand the pipeline further.
